from __future__ import annotations
import argparse, csv, time, os, json
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List

# --- Core importy (nasze modu≈Çy) ---
from djlib.config import (
    reconfigure, ensure_base_dirs, CONFIG_FILE,
    INBOX_DIR, READY_TO_PLAY_DIR, REVIEW_QUEUE_DIR, LOGS_DIR, CSV_PATH, AUDIO_EXTS, UNSORTED_XLSX
)
from djlib.csvdb import load_records, save_records
from djlib.tags import read_tags, write_tags
from djlib.enrich import suggest_metadata, enrich_online_for_row
from djlib.genre import external_genre_votes, load_taxonomy_map, suggest_bucket_from_votes
from djlib.metadata.genre_resolver import resolve as resolve_genres
from djlib.classify import guess_bucket
from djlib.fingerprint import file_sha256, fingerprint_info
from djlib.filename import build_final_filename, extension_for
from djlib.mover import resolve_target_path, move_with_rename, utc_now_str
from djlib.buckets import is_valid_target
from djlib.placement import decide_bucket
from djlib.ml.export_dataset import export_training_dataset
from djlib.taxonomy import load_taxonomy, allowed_targets
from djlib.unsorted import load_unsorted_rows, write_unsorted_rows, is_done
try:
    from djlib.audio import check_env as audio_check_env
    from djlib.audio import analyze as audio_analyze
    from djlib.audio.cache import get_analysis
except Exception:
    # If audio backend is unavailable, fall back to None
    audio_check_env = None  # type: ignore
    audio_analyze = None  # type: ignore
    get_analysis = None  # type: ignore

# --- Pomocnicze ---
REPO_ROOT = Path(__file__).resolve().parents[1]
LEGACY_ML_MSG = (
    "Legacy ML pipeline (FMA) zosta≈Ç usuniƒôty. "
    "Trenowanie i predykcje ML wr√≥cƒÖ po wdro≈ºeniu lokalnych modeli na bazie Essentia. "
    "Na razie skorzystaj z `ml-export-training-dataset`, aby przygotowaƒá CSV do dalszej pracy."
)


def _safe_str(value: Any) -> str:
    if value is None:
        return ""
    if isinstance(value, str):
        return value
    return str(value)


def _load_unsorted() -> List[Dict[str, str]]:
    return load_unsorted_rows(UNSORTED_XLSX)


def _save_unsorted(rows: List[Dict[str, str]]) -> None:
    try:
        choices = allowed_targets()
    except Exception:
        choices = []
    write_unsorted_rows(UNSORTED_XLSX, rows, choices)

# ============ KOMENDY ============

def cmd_configure(_: argparse.Namespace) -> None:
    cfg, path = reconfigure()
    ensure_base_dirs()
    print(f"\n‚úÖ Zapisano konfiguracjƒô do: {path}")
    print(f"   library_root: {cfg.library_root}")
    print(f"   inbox_dir:    {cfg.inbox_dir}\n")

def cmd_scan(_: argparse.Namespace) -> None:
    ensure_base_dirs()
    library_rows = load_records(CSV_PATH)
    staging_rows = _load_unsorted()
    known_hashes = {r.get("file_hash", "") for r in library_rows if r.get("file_hash")}
    known_fps = {r.get("fingerprint", "") for r in library_rows if r.get("fingerprint")}
    known_hashes.update({r.get("file_hash", "") for r in staging_rows if r.get("file_hash")})
    known_fps.update({r.get("fingerprint", "") for r in staging_rows if r.get("fingerprint")})

    status_path = LOGS_DIR / "scan_status.json"

    def _write_status(data: Dict[str, Any]) -> None:
        try:
            LOGS_DIR.mkdir(parents=True, exist_ok=True)
            with status_path.open("w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False)
        except Exception:
            pass

    all_files = [p for p in INBOX_DIR.glob("**/*") if p.is_file() and p.suffix.lower() in AUDIO_EXTS]
    total = len(all_files)
    processed = 0
    added = 0
    errors = 0
    missing_fpcalc = False
    _write_status(
        {
            "state": "running",
            "total": total,
            "processed": 0,
            "added": 0,
            "errors": 0,
            "last_file": "",
        }
    )

    new_rows: List[Dict[str, str]] = []
    for p in all_files:
        fhash = file_sha256(p)
        if fhash in known_hashes:
            processed += 1
            _write_status(
                {
                    "state": "running",
                    "total": total,
                    "processed": processed,
                    "added": added,
                    "errors": errors,
                    "last_file": str(p),
                }
            )
            continue

        tags = read_tags(p)
        try:
            dur, fp = fingerprint_info(p)
        except Exception as e:
            fp = ""
            dur = 0
            errors += 1
            if "fpcalc" in str(e).lower():
                missing_fpcalc = True

        is_dup = "true" if (fp and fp in known_fps) else "false"

        ai_bucket, ai_comment = guess_bucket(
            tags["artist"], tags["title"], tags["bpm"], tags["genre"], tags["comment"]
        )

        sugg = suggest_metadata(p, tags)
        if (sugg.get("duration_suggest") or "").strip() == "" and dur:
            mm = dur // 60
            ss = dur % 60
            sugg["duration_suggest"] = f"{mm}:{ss:02d}"

        track_id = f"{fhash[:12]}_{int(time.time())}"
        rec: Dict[str, str] = {
            "track_id": track_id,
            "file_path": str(p),
            "file_hash": fhash,
            "fingerprint": fp,
            "added_date": utc_now_str(),
            "is_duplicate": is_dup,
            "artist": _safe_str(tags.get("artist")).strip(),
            "title": _safe_str(tags.get("title")).strip(),
            "version_info": _safe_str(tags.get("version_info")).strip(),
            "genre": _safe_str(tags.get("genre")).strip(),
            "bpm": _safe_str(tags.get("bpm")),
            "key_camelot": _safe_str(tags.get("key_camelot")),
            "energy_hint": _safe_str(tags.get("energy_hint")),
            "tag_artist_original": _safe_str(tags.get("artist")),
            "tag_title_original": _safe_str(tags.get("title")),
            "tag_genre_original": _safe_str(tags.get("genre")),
            "tag_bpm_original": _safe_str(tags.get("bpm")),
            "tag_key_original": _safe_str(tags.get("key_camelot")),
            "ai_guess_bucket": _safe_str(ai_bucket),
            "ai_guess_comment": _safe_str(ai_comment),
            "target_subfolder": "",
            "must_play": "",
            "occasion_tags": "",
            "notes": "",
            "pop_playcount": _safe_str(sugg.get("pop_playcount")),
            "pop_listeners": _safe_str(sugg.get("pop_listeners")),
            "meta_source": _safe_str(sugg.get("meta_source")),
            "done": "FALSE",
        }
        for key in [
            "artist_suggest",
            "title_suggest",
            "version_suggest",
            "genre_suggest",
            "album_suggest",
            "year_suggest",
            "duration_suggest",
            "genres_musicbrainz",
            "genres_lastfm",
            "genres_soundcloud",
        ]:
            rec[key] = _safe_str(sugg.get(key, ""))
        staging_rows.append(rec)
        new_rows.append(rec)
        known_hashes.add(fhash)
        if fp:
            known_fps.add(fp)
        added += 1
        processed += 1
        _write_status(
            {
                "state": "running",
                "total": total,
                "processed": processed,
                "added": added,
                "errors": errors,
                "last_file": str(p),
                "missing_fpcalc": missing_fpcalc,
            }
        )

    if new_rows:
        _save_unsorted(staging_rows)
        print(f"Zeskanowano {len(new_rows)} plik√≥w. Zapisano {UNSORTED_XLSX}.")
    else:
        print("Brak nowych plik√≥w do dodania.")

    _write_status(
        {
            "state": "done",
            "total": total,
            "processed": processed,
            "added": added,
            "errors": errors,
            "missing_fpcalc": missing_fpcalc,
            "unsorted_rows": len(staging_rows),
        }
    )

def _load_rules(path: Path) -> Dict[str, Any]:
    import yaml
    if not path.exists():
        return {"rules": [], "fallbacks": {}}
    with path.open("r", encoding="utf-8") as f:
        return (yaml.safe_load(f) or {"rules": [], "fallbacks": {}})

def _decide_for_row(row: Dict[str, str], rules: Dict[str, Any]) -> str:
    artist = (row.get("artist") or "").lower()
    title  = (row.get("title") or "").lower()
    genre  = (row.get("genre") or "").lower()
    comm   = (row.get("ai_guess_comment") or row.get("comment") or "").lower()
    haystack = " ".join([artist, title, genre, comm])

    for rule in rules.get("rules", []):
        words = [w.lower() for w in rule.get("contains", [])]
        if any(w in haystack for w in words):
            return rule.get("target", "")

    fb = rules.get("fallbacks", {})
    guess = row.get("ai_guess_bucket") or ""
    if guess in fb:
        return fb[guess]
    return fb.get("default", "REVIEW QUEUE/UNDECIDED")

def cmd_auto_decide(args: argparse.Namespace) -> None:
    rules_path = Path(args.rules or (REPO_ROOT / "rules.yml"))
    rules = _load_rules(rules_path)
    rows = _load_unsorted()
    updated = 0

    for r in rows:
        if is_done(r.get("done")):
            continue
        if args.only_empty and (r.get("target_subfolder") or "").strip():
            continue
        proposal = _decide_for_row(r, rules)
        if is_valid_target(proposal):
            r["target_subfolder"] = proposal
            updated += 1

    if updated:
        _save_unsorted(rows)
    print(f"Auto-decide: updated={updated}")

def cmd_auto_decide_smart(_: argparse.Namespace) -> None:
    """Lepsze auto-decide: u≈ºywa heurystyk z djlib.placement z progami ufno≈õci.
    ‚â•0.85: ustaw docelowy kube≈Çek; 0.65..0.85: tylko sugestia (ai_guess_*)."""
    HARDCOMMIT_CONF = 0.85
    SUGGEST_CONF = 0.65
    rows = _load_unsorted()
    set_cnt = sug_cnt = 0
    for r in rows:
        if is_done(r.get("done")):
            continue
        if r.get("target_subfolder"):
            continue
        tgt, conf, reason = decide_bucket(r)
        if not tgt:
            continue
        if conf >= HARDCOMMIT_CONF:
            r["target_subfolder"] = f"READY TO PLAY/{tgt}"
            r["ai_guess_bucket"] = ""
            r["ai_guess_comment"] = f"rule:{reason}; conf={conf:.2f}"
            set_cnt += 1
        elif conf >= SUGGEST_CONF:
            r["ai_guess_bucket"]  = f"READY TO PLAY/{tgt}"
            r["ai_guess_comment"] = f"rule:{reason}; conf={conf:.2f}"
            sug_cnt += 1
    if set_cnt or sug_cnt:
        _save_unsorted(rows)
    print(f"‚úÖ Auto-decide (smart): set={set_cnt}, suggested={sug_cnt}")

def cmd_enrich_online(args: argparse.Namespace) -> None:
    """Wzbogaca metadane (suggest_*) dla pozycji pending korzystajƒÖc z MusicBrainz/AcoustID/Last.fm (+ SoundCloud).
    Prowadzi status w LOGS/enrich_status.json, aby UI mog≈Ço pokazywaƒá postƒôp.
    Nie nadpisuje ju≈º zaakceptowanych. Nie zmienia BPM/Key.
    """
    rows = _load_unsorted()
    force_genres = bool(getattr(args, "force_genres", False))
    todo = [r for r in rows if not is_done(r.get("done"))]
    total = len(todo)
    processed = 0
    changed = 0
    mb_set = 0
    lfm_set = 0
    # Check API credentials presence for diagnostics
    try:
        from djlib.config import get_lastfm_api_key
        _lfm_key_present = bool(get_lastfm_api_key())
    except Exception:
        _lfm_key_present = False

    # status plik
    status_path = LOGS_DIR / "enrich_status.json"
    LOGS_DIR.mkdir(parents=True, exist_ok=True)

    def _now_iso() -> str:
        return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

    # Struktura statusu (rozszerzona zgodnie z ARCHITECTURE.md)
    status_doc = {
        "started_at": _now_iso(),
        "completed_at": "",
        "rows_total": total,
        "rows_processed": 0,
        "updated": 0,
        "state": "running",
        "last_file": "",
        "soundcloud": {
            "client_id_status": "unknown",
            "decision": "pending",  # active | skipped | aborted
            "prompt_shown": False,
            "attempted_requests": 0,
            "timestamp": _now_iso(),
        },
        "sources_counts": {
            "musicbrainz": 0,
            "lastfm": 0,
            "soundcloud": 0,
        }
    }

    def _flush_status() -> None:
        try:
            with status_path.open("w", encoding="utf-8") as f:
                json.dump(status_doc, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

    _flush_status()

    # SoundCloud client id health (informative, does not block)
    sc_health_msg = ""
    try:
        from djlib.metadata.soundcloud import client_id_health
        h = client_id_health()
        if h:
            status_doc["soundcloud"]["client_id_status"] = h.get("status", "unknown")
        sc_health_msg = f"soundcloud_client_id_status={h.get('status')}" if h else ""
        if h and h.get("status") == "ok":
            print("‚úÖ SoundCloud client_id OK")
            if not getattr(args, "skip_soundcloud", False):
                status_doc["soundcloud"]["decision"] = "active"
        elif h and h.get("status") in {"invalid", "error"}:
            print(f"‚ö† SoundCloud client_id: {h.get('message')}")
            if getattr(args, "skip_soundcloud", False):
                status_doc["soundcloud"]["decision"] = "skipped"
            else:
                status_doc["soundcloud"]["prompt_shown"] = True
                _flush_status()
                try:
                    choice = input("Kontynuowaƒá bez SoundCloud? [Y/n]: ").strip().lower()
                except Exception:
                    choice = "y"
                if choice in {"n", "no"}:
                    print("Przerwano na pro≈õbƒô u≈ºytkownika (SoundCloud invalid).")
                    status_doc["soundcloud"]["decision"] = "aborted"
                    status_doc["state"] = "done"
                    status_doc["completed_at"] = _now_iso()
                    _flush_status()
                    return
                else:
                    print("‚Üí Pomi≈Ñ SoundCloud w tym przebiegu.")
                    setattr(args, "skip_soundcloud", True)
                    status_doc["soundcloud"]["decision"] = "skipped"
        elif h and h.get("status") == "missing":
            if getattr(args, "skip_soundcloud", False):
                status_doc["soundcloud"]["decision"] = "skipped"
            else:
                print("‚Ñπ Brak SoundCloud client_id (mo≈ºna ustawiƒá DJLIB_SOUNDCLOUD_CLIENT_ID).")
                status_doc["soundcloud"]["decision"] = "skipped"  # treat missing as skipped
    except Exception:
        pass
    _flush_status()

    # przygotuj mapowanie tag√≥w ‚Üí bucket
    tag_map = load_taxonomy_map()

    for r in rows:
        if is_done(r.get("done")):
            continue
        p = Path(r.get("file_path",""))
        online = enrich_online_for_row(p, r)
        if not online:
            processed += 1
            status_doc["rows_processed"] = processed
            status_doc["updated"] = changed
            status_doc["last_file"] = str(p)
            _flush_status()
            continue
        # regu≈Ça nadpisywania:
        # - zawsze nadpisuj, je≈õli ≈∫r√≥d≈Çem jest AcoustID (najwy≈ºszy priorytet)
        # - w innym przypadku: wype≈Çnij je≈õli puste LUB nadpisz fallback (filename|tags_fallback)
        current_source = (r.get("meta_source") or "").strip().lower()
        online_source = (online.get("meta_source") or "").strip().lower()
        acoustid_wins = online_source.startswith("acoustid")
        allow_override = acoustid_wins or (
            current_source in {"filename|tags_fallback", "filename,tags_fallback", "tags_fallback"}
        ) or not r.get("genre_suggest")  # nadpisz je≈õli genre pusty
        any_change = False
        for k, v in online.items():
            if k in {"artist_suggest","title_suggest","version_suggest","genre_suggest","album_suggest","year_suggest","duration_suggest"}:
                cur = (r.get(k) or "").strip()
                if (not cur and v) or (allow_override and v and cur != v):
                    r[k] = v
                    any_change = True
        # ustaw meta_source je≈õli zrobili≈õmy jakƒÖkolwiek aktualizacjƒô i online poda≈Ç ≈∫r√≥d≈Ço
        if any_change and (online.get("meta_source") or "").strip():
            r["meta_source"] = online["meta_source"]
        
    # Zawsze spr√≥buj wzbogaciƒá gatunki u≈ºywajƒÖc wszystkich ≈∫r√≥de≈Ç (MB + Last.fm + SoundCloud)
        try:
            a = (r.get("artist_suggest") or r.get("artist") or "").strip()
            t = (r.get("title_suggest") or r.get("title") or "").strip()
            v = (
                r.get("version_suggest")
                or r.get("version_info")
                or r.get("parsed_version")
                or ""
            ).strip()
            dur_s = None
            if r.get("duration_suggest"):
                try:
                    dur_parts = r["duration_suggest"].split(":")
                    if len(dur_parts) == 2:
                        dur_s = int(dur_parts[0]) * 60 + int(dur_parts[1])
                except Exception:
                    pass
            
            from djlib.metadata.genre_resolver import resolve as resolve_genres
            genre_res = resolve_genres(
                a,
                t,
                version=v,
                duration_s=dur_s,
                disable_soundcloud=bool(getattr(args, "skip_soundcloud", False)),
            )
            if genre_res and genre_res.confidence >= 0.03:  # lower threshold for missing genres
                # Ustaw 3 gatunki: main + subs
                genres = [genre_res.main] + genre_res.subs[:2]  # max 3 total
                genre_str = ", ".join(genres)
                current_genre = (r.get("genre_suggest") or "").strip()
                if not current_genre or genre_res.confidence > 0.08:  # override existing only if significantly better
                    r["genre_suggest"] = genre_str
                    any_change = True
                    # Update meta_source to reflect all sources used
                    sources = [src for src, _, _ in genre_res.breakdown]
                    if sources:
                        r["meta_source"] = f"{r.get('meta_source', '')}+genres({','.join(sources)})".strip("+")

                # Zapisz surowe listy tag√≥w per ≈∫r√≥d≈Ço do dodatkowych kolumn
                try:
                    src_map = {src: local for (src, _, local) in genre_res.breakdown}
                    def _top_k(d, k=5):
                        return ", ".join([kv[0] for kv in sorted(d.items(), key=lambda kv: kv[1], reverse=True)[:k]])
                    if src_map.get("musicbrainz") and (force_genres or not (r.get("genres_musicbrainz") or "")):
                        r["genres_musicbrainz"] = _top_k(src_map["musicbrainz"])  # type: ignore[index]
                        any_change = True
                        mb_set += 1
                    if src_map.get("lastfm") and (force_genres or not (r.get("genres_lastfm") or "")):
                        r["genres_lastfm"] = _top_k(src_map["lastfm"])  # type: ignore[index]
                        any_change = True
                        lfm_set += 1
                    if src_map.get("soundcloud") and (force_genres or not (r.get("genres_soundcloud") or "")):
                        r["genres_soundcloud"] = _top_k(src_map["soundcloud"])  # type: ignore[index]
                        any_change = True
                except Exception:
                    pass
        except Exception as e:
            # Debug: print exception for troubleshooting
            print(f"Genre resolution failed for {a} - {t}: {e}")
            pass

        # Popularno≈õƒá z Last.fm (playcount/listeners) ‚Äî pomoc dla singalong/party dance/decades
        try:
            a = (r.get("artist_suggest") or r.get("artist") or "").strip()
            t = (r.get("title_suggest") or r.get("title") or "").strip()
            if a and t:
                from djlib.metadata.lastfm import track_info as lf_track_info
                info = lf_track_info(a, t) or {}
                if info:
                    # Zapisz pola popularno≈õci, nie nadpisuj istniejƒÖcych >0
                    if info.get("playcount") and int(info.get("playcount", 0)) > int(r.get("pop_playcount", 0) or 0):
                        r["pop_playcount"] = str(info["playcount"])  # zapis w CSV jako string
                    if info.get("listeners") and int(info.get("listeners", 0)) > int(r.get("pop_listeners", 0) or 0):
                        r["pop_listeners"] = str(info["listeners"])  # zapis w CSV jako string
        except Exception:
            pass

        # Zaproponuj kube≈Çek na podstawie gatunk√≥w
        try:
            genre_str = (r.get("genre_suggest") or "").strip()
            if genre_str and tag_map:
                # Parse genres back to individual tags for voting
                genre_tags = [g.strip() for g in genre_str.split(",") if g.strip()]
                votes = {tag: 1.0 for tag in genre_tags}  # equal weight for each genre
                bucket, conf, breakdown = suggest_bucket_from_votes(votes, tag_map)
                if bucket and conf >= 0.65:
                    r["ai_guess_bucket"]  = f"READY TO PLAY/{bucket}"
                    # zbuduj kr√≥tki komentarz z top tag√≥w
                    top_tags = [tag for tag, _, mapped in breakdown if mapped][:3]
                    tags_str = ", ".join(top_tags) if top_tags else genre_str.split(",")[0]
                    r["ai_guess_comment"] = f"genres; conf={conf:.2f}; tags: {tags_str}"
                    any_change = True
        except Exception:
            pass

        if any_change:
            changed += 1
        # Auto-fill artist/title if still empty and we now have suggest values (quality-of-life)
        if not (r.get("artist") or "").strip() and (r.get("artist_suggest") or "").strip():
            r["artist"] = r["artist_suggest"]
        if not (r.get("title") or "").strip() and (r.get("title_suggest") or "").strip():
            r["title"] = r["title_suggest"]
        processed += 1
        status_doc["rows_processed"] = processed
        status_doc["updated"] = changed
        status_doc["last_file"] = str(p)
        _flush_status()
    if changed:
        _save_unsorted(rows)
    # Oblicz ≈∫r√≥d≈Ça u≈ºycia na podstawie wype≈Çnionych kolumn per-source
    mb_cnt = lfm_cnt = sc_cnt = 0
    for r in rows:
        if r.get("genres_musicbrainz"):
            mb_cnt += 1
        if r.get("genres_lastfm"):
            lfm_cnt += 1
        if r.get("genres_soundcloud"):
            sc_cnt += 1
    status_doc["sources_counts"] = {
        "musicbrainz": mb_cnt,
        "lastfm": lfm_cnt,
        "soundcloud": sc_cnt,
    }
    # Uzupe≈Çnij attempted_requests z modu≈Çu SoundCloud
    try:
        from djlib.metadata.soundcloud import soundcloud_request_count
        status_doc["soundcloud"]["attempted_requests"] = soundcloud_request_count()
    except Exception:
        pass
    status_doc["rows_processed"] = processed
    status_doc["updated"] = changed
    status_doc["state"] = "done"
    status_doc["completed_at"] = _now_iso()
    _flush_status()
    print(f"üîé Enrich online: updated={changed}")
    # Short diagnostics
    if total:
        print(f"   ‚Üí genres set ‚Äî MB:{mb_set}, LFM:{lfm_set}")
    if not _lfm_key_present:
        print("   ‚ö† Brak LASTFM_API_KEY (DJLIB_LASTFM_API_KEY) ‚Äî kolumna genres_lastfm mo≈ºe pozostaƒá pusta.")
    if sc_health_msg:
        print(f"   ‚Ñπ {sc_health_msg}")

def cmd_fix_fingerprints(_: argparse.Namespace) -> None:
    """Uzupe≈Çnij brakujƒÖce fingerprinty w istniejƒÖcym CSV.
    Dla ka≈ºdego wiersza bez fingerprintu spr√≥buj wyliczyƒá go z pliku (preferuj final_path, potem file_path).
    Aktualizuj te≈º duration_suggest je≈õli puste.
    Pisz postƒôp do LOGS/fingerprint_status.json, aby UI mog≈Ço pokazywaƒá pasek.
    """
    from djlib.config import LOGS_DIR
    rows = _load_unsorted()
    targets = []
    for r in rows:
        if is_done(r.get("done")):
            continue
        fp = (r.get("fingerprint") or "").strip()
        if fp:
            continue
        # preferuj final_path je≈õli istnieje, w przeciwnym razie file_path
        p = None
        fp1 = r.get("final_path") or ""
        fp2 = r.get("file_path") or ""
        f1 = Path(fp1) if fp1 else None
        f2 = Path(fp2) if fp2 else None
        if f1 and f1.exists():
            p = f1
        elif f2 and f2.exists():
            p = f2
        if p is not None:
            targets.append((r, p))

    total = len(targets)
    processed = 0
    updated = 0
    errors = 0

    status_path = LOGS_DIR / "fingerprint_status.json"

    def _write_status(state: str, last_file: str = "") -> None:
        try:
            LOGS_DIR.mkdir(parents=True, exist_ok=True)
            with status_path.open("w", encoding="utf-8") as f:
                json.dump({
                    "state": state,
                    "total": total,
                    "processed": processed,
                    "updated": updated,
                    "errors": errors,
                    "last_file": last_file,
                }, f, ensure_ascii=False)
        except Exception:
            pass

    _write_status("running", "")

    for r, p in targets:
        try:
            dur, fp = fingerprint_info(p)
            if fp:
                r["fingerprint"] = fp
                # uzupe≈Çnij duration_suggest je≈õli brak
                ds = (r.get("duration_suggest") or "").strip()
                if not ds and dur:
                    mm, ss = divmod(int(dur), 60)
                    r["duration_suggest"] = f"{mm}:{ss:02d}"
                updated += 1
        except Exception:
            errors += 1
        processed += 1
        _write_status("running", str(p))

    if updated:
        _save_unsorted(rows)
    _write_status("done", "")
    print(f"üß© Fix fingerprints: updated={updated}, errors={errors}")

def cmd_fix_titles_from_filenames(_: argparse.Namespace) -> None:
    """Napraw rekordy z pustym/niew≈Ça≈õciwym artist/title korzystajƒÖc z nazwy pliku."""
    from djlib.filename import parse_from_filename
    rows = _load_unsorted()
    if not rows:
        print("Brak rekord√≥w do korekty.")
        return

    def _should_replace(current: str | None, base_tokens: set[str]) -> bool:
        v = (current or "").strip()
        if not v:
            return True
        low = v.lower()
        if low.isdigit():
            return True
        if low.startswith("track") and len(low.split()) <= 2:
            return True
        if base_tokens:
            cur_tokens = {tok for tok in low.split() if len(tok) > 1}
            if not cur_tokens:
                return True
            # je≈õli brak wsp√≥lnych token√≥w z nazwƒÖ pliku ‚Äì traktuj jako b≈Çƒôdne
            if cur_tokens.isdisjoint(base_tokens):
                return True
        return False

    updated = 0
    for r in rows:
        if is_done(r.get("done")):
            continue
        fp = (r.get("file_path") or "").strip()
        if not fp:
            continue
        p = Path(fp)
        a, t, v = parse_from_filename(p)
        if not a and not t:
            continue
        changed = False
        base_artist_tokens = {tok for tok in a.lower().split() if len(tok) > 1}
        base_title_tokens = {tok for tok in t.lower().split() if len(tok) > 1}
        if _should_replace(r.get("artist"), base_artist_tokens) and a:
            r["artist"] = a
            changed = True
        if _should_replace(r.get("title"), base_title_tokens) and t:
            r["title"] = t
            changed = True
        if _should_replace(r.get("artist_suggest"), base_artist_tokens) and a:
            r["artist_suggest"] = a
            changed = True
        if _should_replace(r.get("title_suggest"), base_title_tokens) and t:
            r["title_suggest"] = t
            changed = True
        if not (r.get("version_info") or "").strip() and v:
            r["version_info"] = v
            changed = True
        if not (r.get("version_suggest") or "").strip() and v:
            r["version_suggest"] = v
            changed = True
        if changed:
            meta = (r.get("meta_source") or "").strip()
            if "fix_filename" not in meta:
                r["meta_source"] = (meta + "+fix_filename").strip("+")
            updated += 1

    if updated:
        _save_unsorted(rows)
    print(f"üõ†Ô∏è  Fix titles from filenames: updated={updated}")

def cmd_apply(args: argparse.Namespace) -> None:
    rows = _load_unsorted()
    ready = [r for r in rows if is_done(r.get("done"))]
    if not ready:
        print("Brak wierszy z oznaczeniem done=TRUE.")
        return
    library_rows = load_records(CSV_PATH)
    processed_ids: set[str] = set()
    tags_written = 0
    tags_errors = 0

    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    stamp = time.strftime("%Y%m%d-%H%M%S")
    log_path = LOGS_DIR / f"moves-{stamp}.csv"
    log_rows = []

    for r in ready:
        target = (r.get("target_subfolder") or "").strip()
        if not target:
            continue
        src = Path(r.get("file_path") or "")
        if not src.exists():
            print(f"[WARN] Nie znaleziono pliku: {src}")
            continue

        dest_dir = resolve_target_path(target)
        if dest_dir is None:
            continue

        final_name = build_final_filename(
            r.get("artist") or r.get("tag_artist_original") or r.get("artist_suggest") or "",
            r.get("title") or r.get("tag_title_original") or r.get("title_suggest") or "",
            r.get("version_info") or r.get("version_suggest") or "",
            r.get("key_camelot", ""),
            r.get("bpm", ""),
            extension_for(src),
        )

        dest_path = dest_dir / final_name
        print(f"{'DRY-RUN ' if args.dry_run else ''}MOVE: {src} -> {dest_path}")

        if args.dry_run:
            continue

        dest_real = move_with_rename(src, dest_dir, final_name)
        log_rows.append([str(src), str(dest_real), r.get("track_id", "")])
        processed_ids.add(r.get("track_id", ""))
        record = {
            "track_id": r.get("track_id", ""),
            "file_path": str(dest_real),
            "original_path": r.get("file_path") or "",
            "file_hash": r.get("file_hash") or "",
            "fingerprint": r.get("fingerprint") or "",
            "added_date": utc_now_str(),
            "final_filename": final_name,
            "final_path": str(dest_real),
            "artist": r.get("artist") or r.get("tag_artist_original") or r.get("artist_suggest") or "",
            "title": r.get("title") or r.get("tag_title_original") or r.get("title_suggest") or "",
            "version_info": r.get("version_info") or r.get("version_suggest") or "",
            "genre": r.get("genre") or r.get("genre_suggest") or "",
            "bpm": r.get("bpm") or "",
            "key_camelot": r.get("key_camelot") or "",
            "energy_hint": r.get("energy_hint") or "",
            "target_subfolder": target,
            "must_play": r.get("must_play") or "",
            "occasion_tags": r.get("occasion_tags") or "",
            "notes": r.get("notes") or "",
            "is_duplicate": r.get("is_duplicate") or "",
            "pop_playcount": r.get("pop_playcount") or "",
            "pop_listeners": r.get("pop_listeners") or "",
        }
        library_rows.append(record)
        # Po udanym przeniesieniu spr√≥buj zapisaƒá zaakceptowane metadane do tag√≥w audio
        try:
            updates = {}
            artist = (record["artist"] or "").strip()
            title_base = (record["title"] or "").strip()
            version_info = (record["version_info"] or "").strip()
            if title_base and version_info:
                parts = [p.strip() for p in version_info.split(",") if p.strip()]
                if parts:
                    title_out = title_base + " " + " ".join(f"({p})" for p in parts)
                else:
                    title_out = title_base
            else:
                title_out = title_base
            if artist:
                updates["artist"] = artist
            if title_out:
                updates["title"] = title_out
            genre = (record["genre"] or "").strip()
            if genre:
                updates["genre"] = genre
            bpm_raw = (record["bpm"] or "").strip()
            if bpm_raw:
                try:
                    bpm_val = int(round(float(bpm_raw)))
                    updates["bpm"] = str(bpm_val)
                except Exception:
                    updates["bpm"] = bpm_raw
            key_cam = (record["key_camelot"] or "").strip().upper()
            if key_cam:
                updates["key_camelot"] = key_cam
            if updates:
                write_tags(dest_real, updates)
                tags_written += 1
        except Exception as e:
            print(f"[WARN] Tag write failed for {dest_real}: {e}")
            tags_errors += 1

    if args.dry_run:
        print(f"[DRY-RUN] Gotowe do eksportu: {len(ready)} (oznaczone done=TRUE).")
        return

    if log_rows:
        with log_path.open("w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["src_before", "dest_after", "track_id"])
            w.writerows(log_rows)
        print(f"Zapisano log: {log_path}")

    remaining = [r for r in rows if r.get("track_id") not in processed_ids]
    _save_unsorted(remaining)
    save_records(CSV_PATH, library_rows)
    print(f"Przeniesiono {len(processed_ids)} pozycji do biblioteki.")
    print(f"üìÄ Zapis tag√≥w audio: ok={tags_written}, errors={tags_errors}")

def scan_command() -> None:
    """Funkcja wywo≈ÇujƒÖca skanowanie (u≈ºywana przez webapp i inne modu≈Çy)."""
    args = argparse.Namespace()
    cmd_scan(args)

def cmd_undo(_: argparse.Namespace) -> None:
    logs = sorted(LOGS_DIR.glob("moves-*.csv"))
    if not logs:
        print("Brak log√≥w do cofniƒôcia.")
        return
    log = logs[-1]
    print(f"Cofam ruchy z: {log.name}")

    rows = list(csv.DictReader(log.open("r", encoding="utf-8")))
    reverted = 0
    for r in rows:
        src_before = Path(r["src_before"])
        dest_after = Path(r["dest_after"])
        if dest_after.exists():
            dest_after.rename(src_before)
            reverted += 1
        else:
            print(f"[WARN] Brak pliku do cofniƒôcia: {dest_after}")
    print(f"Cofniƒôto {reverted} ruch√≥w.")

def cmd_dupes(_: argparse.Namespace) -> None:
    groups: dict[str, list[dict[str,str]]] = {}
    rows = load_records(CSV_PATH)
    for r in rows:
        fp = (r.get("fingerprint") or "").strip()
        if not fp:
            continue
        groups.setdefault(fp, []).append(r)

    out = LOGS_DIR / "dupes.csv"
    with out.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["group_fingerprint", "track_id", "artist", "title", "file_path", "final_path", "file_hash"])
        for fp, items in groups.items():
            if len(items) <= 1:
                continue
            for r in items:
                w.writerow([fp, r.get("track_id",""), r.get("artist",""), r.get("title",""),
                            r.get("file_path",""), r.get("final_path",""), r.get("file_hash","")])
    print(f"Zapisano raport duplikat√≥w: {out}")

def cmd_sync_audio_metrics(args: argparse.Namespace) -> None:
    """Zsynchronizuj metryki (BPM/Key/Energy) z cache SQLite do g≈Ç√≥wnego CSV.
    Domy≈õlnie uzupe≈Çnia tylko puste pola; u≈ºyj --force aby nadpisaƒá istniejƒÖce.
    Opcjonalnie zapisuje metadane do plik√≥w audio je≈õli --write-tags.
    """
    if get_analysis is None:
        print("Audio cache backend niedostƒôpny.")
        return
    rows = _load_unsorted()
    if not rows:
        print("Brak rekord√≥w do aktualizacji.")
        return
    force = bool(getattr(args, "force", False))
    write_tags_flag = bool(getattr(args, "write_tags", False))
    updated = 0
    tags_written = 0
    for r in rows:
        audio_id = (r.get("file_hash") or "").strip()
        if not audio_id:
            # spr√≥buj policzyƒá hash je≈õli plik istnieje
            try:
                p = Path(r.get("file_path") or "")
                if p.exists():
                    from djlib.audio.cache import compute_audio_id as _cmp
                    audio_id = _cmp(p)
                else:
                    continue
            except Exception:
                continue
        a = get_analysis(audio_id)
        if not a:
            continue
        # przygotuj warto≈õci
        bpm = a.get("bpm")
        key = a.get("key_camelot")
        energy = a.get("energy")
        # uzupe≈Çniaj tylko puste chyba ≈ºe --force
        def _should_set(cur: str) -> bool:
            return force or (not (cur or "").strip())
        changed = False
        if bpm is not None and _should_set(r.get("bpm", "")):
            try:
                r["bpm"] = f"{float(bpm):.2f}".rstrip("0").rstrip(".")
            except Exception:
                r["bpm"] = str(bpm)
            changed = True
        if key and _should_set(r.get("key_camelot", "")):
            r["key_camelot"] = str(key)
            changed = True
        if energy is not None and _should_set(r.get("energy_hint", "")):
            # energy jako 0..1 ‚Üí wpisz procentowo (0..100) lub float; wybierz prosty procent
            try:
                r["energy_hint"] = f"{round(float(energy)*100)}"
            except Exception:
                r["energy_hint"] = str(energy)
            changed = True
        if changed:
            updated += 1
        
        # Zapisz metadane do pliku je≈õli --write-tags
        if write_tags_flag and (bpm or key):
            try:
                p = Path(r.get("file_path") or "")
                if p.exists():
                    from djlib.tags import write_tags
                    tag_updates = {}
                    if bpm is not None:
                        tag_updates["bpm"] = str(bpm)
                    if key:
                        tag_updates["key_camelot"] = str(key)
                    write_tags(p, tag_updates)
                    tags_written += 1
            except Exception as e:
                print(f"[WARN] Nie uda≈Ço siƒô zapisaƒá tag√≥w do {p}: {e}")
    
    if updated:
        _save_unsorted(rows)
    print(f"üîÑ Sync audio metrics: updated={updated}, tags_written={tags_written}")

def cmd_genres_resolve(args: argparse.Namespace) -> None:
    artist = (getattr(args, "artist", None) or "").strip()
    title = (getattr(args, "title", None) or "").strip()
    dur = getattr(args, "duration", None)
    version = (getattr(args, "version", None) or "").strip()
    res = resolve_genres(artist, title, version=version, duration_s=dur)
    if not res:
        print("Brak wynik√≥w z zewnƒôtrznych ≈∫r√≥de≈Ç (MB/LFM/SoundCloud).")
        return
    print(f"Main: {res.main}")
    if res.subs:
        print(f"Subs: {', '.join(res.subs)}")
    print(f"Confidence: {res.confidence:.2f}")
    print("Breakdown:")
    for src, _, local in res.breakdown:
        parts = ", ".join(f"{k}:{v:.2f}" for k, v in sorted(local.items(), key=lambda kv: kv[1], reverse=True)[:5])
        print(f"  - {src}: {parts}")

def cmd_detect_taxonomy(_: argparse.Namespace) -> None:
    """Wykrywa istniejƒÖcƒÖ strukturƒô folder√≥w i zapisuje jako taxonomy.local.yml."""
    from djlib.taxonomy import detect_taxonomy_from_fs, save_taxonomy, load_taxonomy
    from djlib.config import LIB_ROOT

    # Za≈Çaduj istniejƒÖcƒÖ taksonomiƒô
    existing = load_taxonomy()
    existing_ready = set(existing["ready_buckets"])
    existing_review = set(existing["review_buckets"])

    # Wykryj nowƒÖ z filesystem
    detected = detect_taxonomy_from_fs(LIB_ROOT)
    detected_ready = set(detected["ready_buckets"])
    detected_review = set(detected["review_buckets"])

    # Merge: dodaj nowe wykryte, zachowaj istniejƒÖce
    merged_ready = existing_ready | detected_ready
    merged_review = existing_review | detected_review

    merged = {
        "ready_buckets": sorted(merged_ready),
        "review_buckets": sorted(merged_review),
    }

    save_taxonomy(merged)
    print(f"Zaktualizowano taksonomiƒô: {len(merged_ready)} ready buckets, {len(merged_review)} review buckets")
    if merged_ready:
        print("Ready buckets:", ", ".join(merged_ready))
    if merged_review:
        print("Review buckets:", ", ".join(merged_review))


def cmd_taxonomy_backup(_: argparse.Namespace) -> None:
    """Zr√≥b snapshot taksonomii na podstawie realnej struktury folder√≥w (LIB_ROOT) i zapisz do backup√≥w.

    Nie modyfikuje istniejƒÖcego taxonomy.local.yml. Tworzy:
    - taxonomy.local.yml.backup (nadpisywalny snapshot)
    - taxonomy.local.<timestamp>.yml (archiwalny snapshot)
    """
    from djlib.taxonomy import detect_taxonomy_from_fs
    from djlib.config import LIB_ROOT
    import yaml as _yaml

    lib_root = LIB_ROOT
    data = detect_taxonomy_from_fs(lib_root)
    stamp = time.strftime("%Y%m%d-%H%M%S")
    backup_path = REPO_ROOT / "taxonomy.local.yml.backup"
    archive_path = REPO_ROOT / f"taxonomy.local.{stamp}.yml"
    try:
        with backup_path.open("w", encoding="utf-8") as f:
            _yaml.safe_dump(data, f, allow_unicode=True, sort_keys=False)
        with archive_path.open("w", encoding="utf-8") as f:
            _yaml.safe_dump(data, f, allow_unicode=True, sort_keys=False)
        print(f"üì¶ Snapshot zapisany: {backup_path} oraz {archive_path}")
    except Exception as e:
        print(f"[ERR] Nie uda≈Ço siƒô zapisaƒá backupu taksonomii: {e}")

def cmd_analyze_audio(args: argparse.Namespace) -> None:
    """Analiza audio (BPM/Key/Energy) dla INBOX lub wskazanego pliku/katalogu.
    Wyniki zapisywane sƒÖ do cache SQLite (LOGS/audio_analysis.sqlite).
    """
    # Obs≈Çu≈º --check-env
    if getattr(args, "check_env", False):
        if audio_check_env is None:
            print("Essentia backend niedostƒôpny (brak modu≈Çu).")
            return
        info = audio_check_env()
        print(json.dumps(info, ensure_ascii=False, indent=2))
        return

    if audio_analyze is None:
        print("Audio backend niedostƒôpny. Zainstaluj Essentia lub uruchom z --check-env, aby sprawdziƒá ≈õrodowisko.")
        return

    # Zbierz pliki do analizy
    targets = []
    base = Path(getattr(args, "path", "") or INBOX_DIR)
    if base.is_file():
        targets = [base]
    else:
        base = base if base.exists() else INBOX_DIR
        targets = [p for p in base.glob("**/*") if p.is_file() and p.suffix.lower() in AUDIO_EXTS]

    total = len(targets)
    print(f"DEBUG: base={base}, total_targets={total}")  # DEBUG
    processed = 0
    updated = 0
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    status_path = LOGS_DIR / "audio_status.json"

    def _write_status(state: str, last_file: str = "", last_error: str = "") -> None:
        try:
            with status_path.open("w", encoding="utf-8") as f:
                json.dump({
                    "state": state,
                    "total": total,
                    "processed": processed,
                    "updated": updated,
                    "last_file": last_file,
                    "error": last_error,
                }, f, ensure_ascii=False)
        except Exception:
            pass

    # Parsuj zakres BPM
    lo, hi = 80, 180
    tb = getattr(args, "target_bpm", None)
    if tb and ":" in tb:
        try:
            lo, hi = [int(x) for x in tb.split(":", 1)]
        except Exception:
            pass

    _write_status("running", "")
    for p in targets:
        print(f"DEBUG: processing {p}")  # DEBUG
        try:
            res = audio_analyze(p, target_bpm_range=(lo, hi), recompute=bool(args.recompute), config={"target_bpm": [lo, hi]})
            # Je≈õli analyze dokona≈Ço upsert do cache, liczymy jako updated
            if res:
                updated += 1
            _write_status("running", str(p))
        except Exception as e:
            print(f"DEBUG: exception {e}")  # DEBUG
            _write_status("running", str(p), str(e))
        processed += 1

    _write_status("done", "")
    print(f"üéß Analyze-audio: files={total}, analyzed={updated}")

def cmd_ml_predict(_: argparse.Namespace) -> None:
    print(LEGACY_ML_MSG)


def _strip_ready_prefix(target: str) -> str:
    t = (target or "").strip()
    if t.startswith("READY TO PLAY/"):
        return t.split("/", 2)[-1] if "/" in t else t
    if t.startswith("REVIEW QUEUE/"):
        return t.split("/", 2)[-1] if "/" in t else t
    return t


def cmd_ml_train_local(_: argparse.Namespace) -> None:
    print(LEGACY_ML_MSG)


def cmd_ml_export_dataset(args: argparse.Namespace) -> None:
    """Export Essentia features + genre/bucket labels to CSV."""
    out_path = Path(getattr(args, "out", "") or (REPO_ROOT / "data" / "training_dataset_full.csv"))
    require_both = bool(getattr(args, "require_both_labels", False))
    stats = export_training_dataset(out_path=out_path, require_both_labels=require_both)
    print(
        f"ML dataset export: rows={stats['rows_exported']}, "
        f"missing_features={stats['missing_features']}, missing_labels={stats['missing_labels']}"
    )
    print(f" ‚Üí CSV: {stats['output_path']}")


def cmd_qa_acceptance(args: argparse.Namespace) -> None:
    """Policz acceptance rate po predykcjach ML.

    Domy≈õlnie czyta LOGS/ml_predictions.csv i por√≥wnuje z CSV (target_subfolder).
    Zlicza, ile predykcji zosta≈Ço zaakceptowanych (predykcja == docelowy bucket).
    """
    log_path = LOGS_DIR / "ml_predictions.csv"
    if not log_path.exists():
        print("Brak LOGS/ml_predictions.csv ‚Äî najpierw uruchom ml-predict.")
        return
    rows = load_records(CSV_PATH)
    by_path = {r.get("file_path"): r for r in rows}

    import csv as _csv
    total = 0
    accepted = 0
    min_conf = float(getattr(args, "min_confidence", 0.65))
    per_bucket = {}
    with log_path.open("r", encoding="utf-8") as f:
        reader = _csv.DictReader(f)
        for rec in reader:
            try:
                fp = rec.get("file_path") or rec.get("file") or rec.get("path") or rec.get("0") or rec.get("file_path, bucket, confidence")
                # nasze logi majƒÖ prosty format "file_path,bucket,confidence" bez nag≈Ç√≥wk√≥w przy pierwszym zapisie,
                # ale p√≥≈∫niej dok≈Çadamy nag≈Ç√≥wek ‚Äî obs≈Çu≈ºmy oba przypadki
                if not fp and len(rec) == 3:
                    # spr√≥buj odczytu z anonimowych kluczy
                    vals = list(rec.values())
                    fp, pbucket, confs = vals[0], vals[1], vals[2]
                else:
                    pbucket = rec.get("bucket") or ""
                    confs = rec.get("confidence") or "0"
                conf = 0.0
                try:
                    conf = float(confs)
                except Exception:
                    pass
                if conf < min_conf:
                    continue
                total += 1
                r = by_path.get(fp)
                if not r:
                    continue
                tgt = _strip_ready_prefix(r.get("target_subfolder", ""))
                pred = _strip_ready_prefix(pbucket)
                if tgt and pred and tgt == pred:
                    accepted += 1
                    per_bucket[pred] = per_bucket.get(pred, 0) + 1
            except Exception:
                continue

    rate = (accepted / total) if total else 0.0
    print(f"Acceptance: {accepted}/{total} = {rate:.2%}")
    if per_bucket:
        print("Akceptacje per bucket:")
        for b, c in sorted(per_bucket.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {b}: {c}")



# ============ PARSER ============

def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="djlib", description="DJ Library Manager CLI")
    sp = p.add_subparsers(dest="cmd", required=True)

    sp.add_parser("configure").set_defaults(func=cmd_configure)
    sp.add_parser("scan").set_defaults(func=cmd_scan)

    ap = sp.add_parser("auto-decide")
    ap.add_argument("--rules", default=str(REPO_ROOT / "rules.yml"))
    ap.add_argument("--only-empty", action="store_true")
    ap.set_defaults(func=cmd_auto_decide)

    ap2 = sp.add_parser("apply")
    ap2.add_argument("--dry-run", action="store_true")
    ap2.set_defaults(func=cmd_apply)

    sp.add_parser("undo").set_defaults(func=cmd_undo)
    sp.add_parser("dupes").set_defaults(func=cmd_dupes)
    sap = sp.add_parser("sync-audio-metrics")
    sap.add_argument("--force", action="store_true")
    sap.add_argument("--write-tags", action="store_true", help="Zapisz metadane (BPM/Key) do plik√≥w audio")
    sap.set_defaults(func=cmd_sync_audio_metrics)
    sp.add_parser("fix-fingerprints").set_defaults(func=cmd_fix_fingerprints)
    sp.add_parser("fix-filenames").set_defaults(func=cmd_fix_titles_from_filenames)
    ep = sp.add_parser("enrich-online")
    ep.add_argument("--force-genres", action="store_true", help="Nadpisz kolumny genres_musicbrainz/lastfm nawet je≈õli ju≈º wype≈Çnione")
    ep.add_argument("--skip-soundcloud", action="store_true", help="Pomi≈Ñ ≈∫r√≥d≈Ço SoundCloud nawet je≈õli client_id jest ustawiony")
    ep.set_defaults(func=cmd_enrich_online)

    # analyze-audio
    aap = sp.add_parser("analyze-audio")
    aap.add_argument("--path", default=str(INBOX_DIR), help="≈öcie≈ºka pliku lub folderu (domy≈õlnie INBOX)")
    aap.add_argument("--check-env", action="store_true", help="Sprawd≈∫ ≈õrodowisko Essentia")
    aap.add_argument("--recompute", action="store_true", help="Pomi≈Ñ cache i przelicz na nowo")
    aap.add_argument("--workers", type=int, default=1, help="Liczba worker√≥w (na razie ignorowane; skeleton)")
    aap.add_argument("--target-bpm", default="80:180", help="Zakres docelowy BPM, np. 80:180")
    aap.set_defaults(func=cmd_analyze_audio)

    # ml predict
    mp = sp.add_parser("ml-predict")
    mp.add_argument("--model", default=str(REPO_ROOT / "models" / "bucket_model.pkl"))
    mp.add_argument("--path", default=str(INBOX_DIR), help="Plik lub folder (domy≈õlnie INBOX)")
    mp.add_argument("--recompute", action="store_true", help="Przelicz analizƒô audio na nowo")
    mp.add_argument("--set-target", action="store_true", help="Ustawiaj docelowy kube≈Çek powy≈ºej progu hard")
    mp.add_argument("--suggest", action="store_true", help="Ustawiaj tylko ai_guess_* powy≈ºej progu suggest")
    mp.add_argument("--hard-threshold", type=float, default=0.85)
    mp.add_argument("--suggest-threshold", type=float, default=0.65)
    mp.add_argument("--min-confidence", type=float, default=0.40, help="Nie zapisuj ≈ºadnych sugestii poni≈ºej tego progu")
    mp.set_defaults(func=cmd_ml_predict)

    # ml-train-local: trenuj model na Twoich zaakceptowanych bucketach
    tl = sp.add_parser("ml-train-local")
    tl.add_argument("--min-per-class", type=int, default=20, help="Minimalna liczba pr√≥bek na klasƒô (odfiltruj rzadkie)")
    tl.add_argument("--limit", type=int, default=None, help="Limit pr√≥bek do szybkiego treningu (opcjonalnie)")
    tl.add_argument("--recompute", action="store_true", help="Przelicz analizƒô Essentia je≈õli brak w cache")
    tl.add_argument("--out", default=str(REPO_ROOT / "models" / "bucket_model.pkl"))
    tl.set_defaults(func=cmd_ml_train_local)

    ds = sp.add_parser("ml-export-training-dataset")
    ds.add_argument("--out", default=str(REPO_ROOT / "data" / "training_dataset_full.csv"))
    ds.add_argument("--require-both-labels", action="store_true", help="Uwzglƒôdnij tylko rekordy z kompletnymi etykietami")
    ds.set_defaults(func=cmd_ml_export_dataset)

    # QA: acceptance rate
    qa = sp.add_parser("qa-acceptance")
    qa.add_argument("--min-confidence", type=float, default=0.65, help="Licz tylko predykcje powy≈ºej tego progu")
    qa.set_defaults(func=cmd_qa_acceptance)

    # XLSX export/import z dropdownem na bucket
    # genres resolve (single lookup)
    gp = sp.add_parser("genres")
    gsp = gp.add_subparsers(dest="subcmd", required=True)
    res = gsp.add_parser("resolve")
    res.add_argument("--artist", required=True)
    res.add_argument("--title", required=True)
    res.add_argument("--duration", type=int, default=None, help="Duration in seconds (optional)")
    res.add_argument("--version", default="", help="Version/remix info to improve SoundCloud lookup")
    res.set_defaults(func=cmd_genres_resolve)

    sp.add_parser("detect-taxonomy").set_defaults(func=cmd_detect_taxonomy)

    # --- Meta-komendy: round-1 i round-2 ---
    tb = sp.add_parser("taxonomy-backup", help="Zr√≥b snapshot taksonomii na podstawie folder√≥w i zapisz backup")
    tb.set_defaults(func=cmd_taxonomy_backup)
    return p

def main() -> None:
    parser = build_parser()
    args = parser.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()
